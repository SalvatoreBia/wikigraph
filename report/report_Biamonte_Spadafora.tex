\documentclass[conference, a4paper]{IEEEtran} % <--- CAMBIA QUI IN CONFERENCE

\usepackage[utf8]{inputenc}
\usepackage[T1]{fontenc}
\usepackage[english]{babel}
\usepackage{graphicx}
\usepackage{lipsum}
\usepackage{titlesec}

% Modifica lo stile delle SEZIONI (es. I. INTRODUCTION)
% \bfseries = grassetto, \Large = grande
\titleformat{\section}
  {\normalfont\Large\bfseries} % Stile del testo
  {\thesection.}               % Numero della sezione (es. I.)
  {1em}                        % Spazio tra numero e titolo
  {}


\title{Real-time Vandalism Detection in Wikipedia Streams using Graph Mining and Large Language Models}

% Blocco Autori con sintassi specifica IEEE Conference
\author{
    \IEEEauthorblockN{Biamonte Salvatore}
    \IEEEauthorblockA{
    Mat. 264177}

    \and % <--- Questo comando separa i due blocchi
    
    \IEEEauthorblockN{Spadafora Pierpaolo}
    \IEEEauthorblockA{
    Mat. 543210}
}

\date{\today}

\begin{document}
\maketitle

\thispagestyle{plain}
\pagestyle{plain}

\begin{abstract}
The rapid growth of user-generated content on Wikipedia poses significant challenges for maintaining information integrity. The high velocity of edits makes manual moderation unfeasible, necessitating automated real-time solutions. This paper presents a microservices-based architecture designed to detect vandalism in Wikipedia streams using a hybrid approach of Graph Mining and Artificial Intelligence. 
We leverage Graph Data Science techniques (Leiden algorithm) to cluster pages into communities, using edit bursts as triggers for anomaly detection. We implement a Retrieval-Augmented Generation (RAG) pipeline backed by a Neo4j Knowledge Graph to provide factual context to the classifiers.
Finally, we conduct a comparative analysis and an ablation study between a high-precision Large Language Model (Gemini) acting as an oracle, and lightweight Neural Network classifiers trained on synthetic adversarial data. Our results highlight the trade-offs between the computational cost of LLMs and the speed of neural classifiers enhanced by graph-based RAG features.
\end{abstract}

\vspace{1cm}

\section{Introduction}
Wikipedia stands out as one of the most significant sources of crowdsourced knowledge in the digital age. Its open nature allows for the democratization of information, enabling any user to contribute to historical and scientific documentation. However, this accessibility has an obvious downside: the platform is inherently vulnerable to malicious actors. Vandalism, misinformation, and the injection of unverified facts can occur at any moment, potentially compromising the integrity of the encyclopedia.



A useful tool for monitoring such behaviour is the real-time \texttt{RecentChanges} API, provided by WikiMedia, which broadcasts every modification made across the platform. While this tool offers transparency, the sheer volume and velocity of the incoming data make manual verification impossible. Human moderators cannot keep up with the flood of edits that occur every second. Consequently, there is a critical need for autonomous systems capable of filtering this stream and flagging suspicious activities in real-time.

This project addresses this challenge by leveraging \textbf{Graph Mining} techniques to optimize the detection workflow. We construct a knowledge graph utilizing Neo4j to capture the topological structure of Wikipedia. By applying community detection algorithms (such as Leiden), we segment the graph into topical clusters; detecting a sudden spike of edits within a specific cluster allows the system to prioritize resources on potential coordinated attacks.

Building upon this structural foundation, this work conducts a comparative analysis between two distinct detection paradigms: \textbf{Feed-Forward Neural Networks (FNN)} and modern \textbf{Large Language Models (LLMs)}. specifically, we perform an \textbf{Ablation Study} on the Neural classifiers to evaluate the impact of Retrieval-Augmented Generation (RAG) features, extracted from the graph, on the model's accuracy. The primary objective is to evaluate the trade-offs between these approaches, determining which methodology offers the most effective solution for real-time vandalism detection.

In the upcoming sections, we will detail the graph construction methodology and the experimental setup. Finally, we will describe the implemented anomaly detection architectures and discuss the analysis of our findings.

\vspace{0.5cm}

\section{Graph Construction Methodology}
\noindent The construction of the Knowledge Graph is a multi-stage process that transforms a raw Wikipedia dump into a rich, queryable structure within Neo4j. This process involves data cleaning, topological sampling, community detection, and semantic enrichment.

\subsection{Data Sourcing and Sampling}
We primarily use two datasets provided by the Wikimedia Foundation: 
\begin{itemize}
    \item link graph dump (containing page-to-page references) 
    \item the article content dump (containing the full text of the page).
\end{itemize}
Given the prohibitive size of the complete Wikipedia graph for in-memory graph projections, we applied a dimensionality reduction strategy. First, the raw link data was parsed into a structured CSV format $(source, destination)$, filtering out redirects to retain only direct, semantic page references.

Subsequently, to generate a manageable yet representative dataset for the Graph Data Science (GDS) engine, we employed \textbf{Snowball Sampling} technique with a depth of $K=2$, resulting in a coherent subgraph suitable for the experiments.

\subsection{Topological Initialization}
The sampled dataset is ingested into \textbf{Neo4j}. At this stage, the graph is purely structural: nodes represent pages (identified solely by their ID) and edges represent hyperlinks.

\subsection{Community Detection}
Once the topology is established, we enrich the nodes with structural cluster information using the \textbf{Neo4j Graph Data Science (GDS)} library. We integrated three distinct community detection algorithms to capture different granularities of network structure: \textit{Label Propagation}, \textit{Louvain}, and \textit{Leiden}.

This structural partitioning serves a functional purpose in the real-time pipeline: it acts as a heuristic trigger. By monitoring the frequency of edits within specific communities, the system can detect abnormal bursts of localized activity (e.g., a coordinated attack on a specific topic). If the edit rate within a single community exceeds a predefined threshold, the system activates the anomaly detection modules, thereby optimizing resource usage.

\subsection{Semantic Enrichment}
The next phase involves populating the graph with semantic content. We process the \textit{pages-articles} XML dump to associate text with the structural nodes. Since Wikipedia stores data in MediaWiki markup, we first perform a parsing and cleaning step to extract the plain text and the title of each article. This processed data is exported to a secondary CSV file containing tuples of $(page\_id, title, content)$. Finally, these attributes are mapped to the existing nodes in Neo4j, updating the graph schema to include the textual context required for the subsequent analysis.

\subsection{Vector Indexing and RAG Preparation}
To enable the semantic search capabilities required by the LLM, we extended the graph schema to support vector operations.
A key design objective was to mitigate the inherent limitations of LLMs, specifically regarding knowledge cutoffs and hallucination. We envision a deployment scenario where the platform establishes data-sharing agreements with authoritative publishers (e.g., news agencies, scientific journals). These partners provide a stream of verified articles, serving as an external ground truth to validate Wikipedia edits against real-time facts.

To support this dual-source retrieval, we implemented a uniform \textbf{chunking strategy}. The textual content is segmented into smaller overlapping blocks using a sliding window approach (e.g., chunk size of 512 tokens with an overlap of 50 tokens). This overlap is critical to preserve semantic continuity across segment boundaries.

Each text segment is encoded into a high-dimensional vector using a pre-trained Transformer model (\texttt{paraphrase-multilingual-MiniLM-L12-v2}). In Neo4j, these vectors are materialized as distinct nodes to facilitate targeted retrieval:
\begin{itemize}
    \item \texttt{:Chunk} nodes, linked to the original Wikipedia article via a \texttt{[:HAS\_CHUNK]} relationship.
    \item \texttt{:TrustedChunk} nodes, linked to the external authoritative source via a \texttt{[:HAS\_TRUSTED\_CHUNK]} relationship.
\end{itemize}

Finally, specific Vector Indexes are created on both node types using cosine similarity. This dual-indexing architecture enables the Retrieval-Augmented Generation (RAG) pipeline to efficiently fetch both the potentially vandalized content and the authoritative ground truth context during the detection phase.


\vspace{0.5cm}

\section{Vandalism Detection Architectures}
We implemented two opposing approaches to classification: a resource-intensive "AI Judge" based on Generative AI, and a suite of lightweight Neural Classifiers designed for high-throughput scoring.

\subsection{The LLM Oracle (AI Judge)}
The first approach utilizes \textbf{Google Gemini} as a zero-shot classifier. The system constructs a prompt containing the user's edit comment, the original text, the modified text, and the "Ground Truth" retrieved via RAG from our Trusted Sources. The LLM is tasked with reasoning whether the edit is legitimate or vandalism based on factual consistency. While highly accurate, this approach introduces significant latency and API costs.

\subsection{Neural Classifiers and Ablation Study}
To achieve real-time performance, we trained a set of \textbf{Feed-Forward Neural Networks} using PyTorch. The architecture consists of a multi-layer perceptron (MLP) with three hidden layers (256, 128, 64 units), employing Batch Normalization, Dropout (0.5), and ReLU activation functions, culminating in a Sigmoid output for binary classification.

We conducted an \textbf{Ablation Study} to determine the importance of context (RAG). We trained five distinct variations of the model:
\begin{enumerate}
    \item \textbf{Neural Complete:} Uses all features: embeddings of the old text, new text, user comment, and—crucially—the cosine similarity scores (RAG scores) retrieved from the Neo4j Vector Index (triangulation between Wikipedia and Trusted Sources).
    \item \textbf{No RAG:} Excludes the similarity scores from the Knowledge Graph, relying only on the text embeddings.
    \item \textbf{No Comment:} Ignores the user's edit summary.
    \item \textbf{Only New:} Analyzes only the embedding of the resulting text.
    \item \textbf{Minimal (Baseline):} Uses only metadata features (text length ratio), serving as a naive baseline.
\end{enumerate}


\vspace{0.5cm}


\section{Experimental Setup}
To ensure reproducibility and manage computational overhead, we selected the \textbf{Italian Wikipedia} snapshot as our reference dataset. The experiments were conducted using a containerized microservices architecture.

\subsection{Dataset Generation}
We utilized the official Wikimedia dumps for the Italian language (specifically \texttt{itwiki-latest-page.sql}, \texttt{itwiki-latest-pagelinks.sql}, and \texttt{itwiki-latest-pages-articles.xml}).

From this snapshot, we generated a synthetic labeled dataset following the pipeline described in the previous section. To maintain a strict class balance, the dataset is partitioned as follows:
\begin{itemize}
    \item \textbf{Training Set:} 200 samples (100 Legitimate, 100 Vandalism).
    \item \textbf{Testing Set:} 40 samples (20 Legitimate, 20 Vandalism).
\end{itemize}

It is important to note that the dataset size is intentionally constrained due to the high computational (and monetary) cost of the LLM generation, hence this work is explicitly framed as a Proof of Concept. The primary objective is to validate the architectural feasibility of the Graph-RAG pipeline and the integration of microservices, rather than to achieve large-scale statistical generalization.

\subsection{System Infrastructure}
The system is deployed via \textbf{Docker Compose} to ensure isolation. To handle the real-time throughput, we introduced \textbf{Apache Kafka} as a buffering layer: incoming edits are published to a specific topic, allowing the ingestion layer to operate at high velocity while the detection modules process events asynchronously. The graph backend relies on \textbf{Neo4j Community Edition} with the \textit{Graph Data Science (GDS)} plugin enabled for topological analysis.

\subsection{Testing Methodology}
Validating against live streams is challenging due to the sporadic nature of attacks. We therefore developed two injection tools:
\begin{enumerate}
    \item \textbf{Automated Stream Injection:} A Kafka Producer that replays the synthetic test set to measure latency and stress-test the community triggers.
    \item \textbf{Manual Adversarial Testing:} A CLI tool allowing operators to inject custom, human-crafted attacks in real-time, testing the system on more subtle modification.
\end{enumerate}


\section{Results and Discussion}
The experimental results demonstrate a clear hierarchy in performance. 
The \textbf{LLM Judge} achieved the highest accuracy, successfully identifying subtle vandalism (e.g., changing dates or names) thanks to the RAG context. However, its processing time (avg. 1-2 seconds per event) is unsuitable for the full Wikipedia firehose.

The \textbf{Neural Complete} model offered the best trade-off. By incorporating the RAG similarity scores as input features, it approached the accuracy of the LLM but with inference times orders of magnitude faster (milliseconds).
Notably, the \textit{Ablation Study} confirmed the value of the Graph-RAG pipeline: the \textbf{No RAG} model showed a significant drop in F1-score compared to the Complete model, proving that "blind" text analysis is insufficient for detecting semantic vandalism. The \textbf{Minimal} baseline performed poorly, confirming that simple heuristics (like text length) are obsolete against modern vandalism.


\end{document}